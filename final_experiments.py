# -*- coding: utf-8 -*-
"""RP_final_experiments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BjUIV9fYVUfAR2TlGKRQjJz-q9WLaSrL
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
tf.config.list_physical_devices('GPU')

import numpy as np

# Load the full dataset
full_data_path = "/content/drive/MyDrive/KaggleData/ProcessedData/full_data.npy"
full_data = np.load(full_data_path, allow_pickle=True)

# Separate the images and labels
images = np.array([item[0] for item in full_data])
labels = np.array([item[1] for item in full_data])

print(f"Images shape: {images.shape}")  # This should show (7909, 224, 224, 3) or similar
print(f"Labels shape: {labels.shape}")  # This should show (7909,)

# You can now proceed with the rest of data preparation and modelling

# Sensitivity and Specificity metrics
def sensitivity(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    return true_positives / (possible_positives + K.epsilon())

def specificity(y_true, y_pred):
    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))
    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))
    return true_negatives / (possible_negatives + K.epsilon())

# Custom weighted binary cross-entropy
def weighted_binary_crossentropy(y_true, y_pred, weight_positive=1.0, weight_negative=1.0):
    y_true_float = tf.cast(y_true, tf.float32)
    bce = tf.keras.losses.binary_crossentropy(y_true_float, y_pred, from_logits=False)
    weight_vector = y_true_float * weight_positive + (1. - y_true_float) * weight_negative
    weighted_bce = weight_vector * bce
    return tf.reduce_mean(weighted_bce)

import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense, Conv2D, MaxPooling2D, BatchNormalization, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
import os
import pickle

# Build model function with binary classification adjustments
def build_resnet50_model():
    resnet50_model = Sequential()
    backbone = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    resnet50_model.add(backbone)
    resnet50_model.add(Conv2D(16, 3, activation="relu", padding="same"))
    resnet50_model.add(BatchNormalization())
    resnet50_model.add(MaxPooling2D())
    resnet50_model.add(Conv2D(32, 3, activation="relu", padding="same"))
    resnet50_model.add(BatchNormalization())
    resnet50_model.add(MaxPooling2D())
    resnet50_model.add(Flatten())
    resnet50_model.add(Dropout(0.5))
    resnet50_model.add(Dense(1, activation='sigmoid'))  # Sigmoid activation for binary classification
    return resnet50_model
    # Function to train the model with data augmentation

def train_resnet50_model(model_function, x_train, y_train, x_val, y_val, epochs, model_save_dir, class_weights):
    # Build and compile the model
    resnet50_model = model_function()
    resnet50_model.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss=lambda y_true, y_pred: weighted_binary_crossentropy(y_true, y_pred, weight_positive=class_weights[1], weight_negative=class_weights[0]),
        metrics=['accuracy', sensitivity, specificity, AUC(name='roc_auc')]
    )

    datagen = ImageDataGenerator(
        preprocessing_function=preprocess_image,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    early_stop = EarlyStopping(monitor='val_loss', patience=5)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)

    train_generator = datagen.flow(x_train, y_train, batch_size=32)
    val_generator = datagen.flow(x_val, y_val, batch_size=16, shuffle=False)

    history = resnet50_model.fit(
        train_generator,
        steps_per_epoch=len(x_train) // 32,
        epochs=epochs,
        validation_data=val_generator,
        validation_steps=len(x_val) // 32,
        class_weight=class_weights,
        callbacks=[early_stop, reduce_lr]
    )

    resnet50_model.save(os.path.join(model_save_dir, "resnet50_model.h5"))
    return resnet50_model, history


# Apply preprocessing to each image
preprocessed_images = np.array([preprocess_image(img) for img in images])
preprocessed_labels = np.array(labels)

# # Define your preprocessed dataset
# images = np.array([i[0] for i in full_data])
# labels = np.array([i[1] for i in full_data])

# Split the dataset into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Define the number of splits for cross-validation
n_splits = 3

# Create a StratifiedKFold object
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize list to store validation scores
validation_scores = []

# Create a new folder for cross-validation results
model_save_dir = "/content/drive/MyDrive/KaggleData/CrossValidationResults"
os.makedirs(model_save_dir, exist_ok=True)

# Loop over each fold
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]

    # Class weights for imbalanced data
    total_samples = len(y_train_fold)
    class_counts = np.unique(y_train_fold, return_counts=True)[1]
    weight_for_0 = total_samples / (2 * class_counts[0])
    weight_for_1 = total_samples / (2 * class_counts[1])
    class_weights = {0: weight_for_0, 1: weight_for_1}

    # Train the model
    resnet50_model, history = train_resnet50_model(build_resnet50_model, x_train_fold, y_train_fold, x_val_fold, y_val_fold, 12, model_save_dir, class_weights)

    # Evaluate the model on validation data
    val_loss, val_accuracy, val_sensitivity, val_specificity, val_auc = resnet50_model.evaluate(x_val_fold, y_val_fold)
    print(f'Validation Loss: {val_loss}')
    print(f'Validation Accuracy: {val_accuracy}')
    print(f'Validation Sensitivity: {val_sensitivity}')
    print(f'Validation Specificity: {val_specificity}')
    print(f'Validation AUC: {val_auc}')

    # Store validation scores
    validation_scores.append((val_loss, val_accuracy, val_sensitivity, val_specificity, val_auc))

# Calculate average validation scores across folds
average_validation_scores = np.mean(validation_scores, axis=0)
print("Average Validation Scores:")
print(f"Loss: {average_validation_scores[0]}")
print(f"Accuracy: {average_validation_scores[1]}")
print(f"Sensitivity: {average_validation_scores[2]}")
print(f"Specificity: {average_validation_scores[3]}")
print(f"AUC: {average_validation_scores[4]}")



# Save model and training history
model_path = os.path.join(model_save_dir, "resnet50_model.h5")  # Ensure the correct model name here
resnet50_model.save(model_path)  # Make sure to use the correct variable (resnet50_model)

history_path = os.path.join(model_save_dir, "resnet50_model_history.pkl")
with open(history_path, 'wb') as f:
    pickle.dump(history.history, f)  # Ensure using the correct history variable

print(f"Resnet50 Model with data augmentation and training history saved to {model_save_dir}")

# # Create a new folder for cross-validation results
# model_save_dir = "/content/drive/MyDrive/KaggleData/CrossValidationResults"
# os.makedirs(model_save_dir, exist_ok=True)

# # Save model and training history
# model_path = os.path.join(model_save_dir, "resnet50.h5")  # Ensure the correct model name here
# resnet50_model.save(model_path)  # Make sure to use the correct variable (resnet50_model)

# history_path = os.path.join(model_save_dir, "resnet50_history.pkl")
# with open(history_path, 'wb') as f:
#     pickle.dump(history.history, f)  # Ensure using the correct history variable

# print(f"Resnet50 Model and training history saved to {model_save_dir}")

#Plotting the model results

#Getting the accuracy
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

#Getting the losses
loss = history.history['loss']
val_loss = history.history['val_loss']

#No of epochs it trained
epochs_range = history.epoch

#Plotting Training and Validation accuracy
plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('ResNet50: Training and Validation Accuracy')

#Plotting Training and Validation Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('ResNet50: Training and Validation Loss')
plt.show()

import os
import pickle
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.metrics import AUC

# Custom Sensitivity and Specificity Metrics
def sensitivity(y_true, y_pred):
    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))
    possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))
    return true_positives / (possible_positives + tf.keras.backend.epsilon())

def specificity(y_true, y_pred):
    true_negatives = tf.reduce_sum(tf.round(tf.clip_by_value((1-y_true) * (1-y_pred), 0, 1)))
    possible_negatives = tf.reduce_sum(tf.round(tf.clip_by_value(1-y_true, 0, 1)))
    return true_negatives / (possible_negatives + tf.keras.backend.epsilon())

# Custom Weighted Binary Crossentropy
def weighted_binary_crossentropy(y_true, y_pred, weight_positive=1.0, weight_negative=1.0):
    y_true_float = tf.cast(y_true, tf.float32)
    bce = tf.keras.losses.binary_crossentropy(y_true_float, y_pred, from_logits=False)
    weight_vector = y_true_float * weight_positive + (1. - y_true_float) * weight_negative
    weighted_bce = weight_vector * bce
    return tf.reduce_mean(weighted_bce)

# Define the path to the directory where the model and history file are saved
model_save_dir = "/content/drive/MyDrive/KaggleData/XVal"
model_path = os.path.join(model_save_dir, "resnet50.h5")


resnet50_model = load_model(model_path, custom_objects={
    '<lambda>': lambda y_true, y_pred: weighted_binary_crossentropy(y_true, y_pred, weight_positive=1.0, weight_negative=1.0),
    'sensitivity': sensitivity,
    'specificity': specificity,
    'AUC': tf.keras.metrics.AUC
})

# Print the model summary to check its architecture
try:
    resnet50_model.summary()
except AttributeError as e:
    print("Model is not loaded correctly:", e)

# Load the training history
history_path = os.path.join(model_save_dir, "resnet50_history.pkl")
if os.path.exists(history_path):
    with open(history_path, 'rb') as f:
        history = pickle.load(f)
    print("Training history successfully loaded.")
else:
    print("History file does not exist.")

# # Optionally, check some details from the loaded history
# if 'history' in locals():
#     print("Loaded history keys:", history.keys())
#     print("Example - Accuracy over epochs:", history.get('accuracy'))

from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
import numpy as np

def calculate_f1_scores(probabilities, true_labels):
    """Calculates F1 scores for a range of thresholds."""
    thresholds = np.arange(0, 1, 0.01)
    f1_scores = [f1_score(true_labels, probabilities > threshold) for threshold in thresholds]
    return thresholds, f1_scores

def find_optimal_threshold(probabilities, true_labels):
    """Finds the threshold that yields the highest F1 score."""
    thresholds, f1_scores = calculate_f1_scores(probabilities, true_labels)
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    optimal_f1_score = f1_scores[optimal_idx]
    return optimal_threshold, optimal_f1_score

predicted_probs = []
actual_labels = []

# Iterate over each fold
for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predicted probabilities from the model for the validation set
    # Reduce the batch size in your predict call
    probs = resnet50_model.predict(x_val_fold, batch_size=16).flatten()  # Adjust the batch size based on your available GPU memory

    predicted_probs.extend(probs)
    actual_labels.extend(y_val_fold)

# Convert lists to numpy arrays for analysis
predicted_probs = np.array(predicted_probs)
actual_labels = np.array(actual_labels)

# Find the optimal threshold and F1 score

optimal_threshold, optimal_f1_score = find_optimal_threshold(predicted_probs, actual_labels)
print(f"Optimal threshold: {optimal_threshold}")
print(f"Optimal F1 Score: {optimal_f1_score}")

# Plot F1 scores across thresholds
thresholds, f1_scores = calculate_f1_scores(predicted_probs, actual_labels)
plt.plot(thresholds, f1_scores)
plt.xlabel('Threshold')
plt.ylabel('F1 Score')
plt.title('ResNet50: Threshold vs F1 Score')
plt.axvline(x=optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold = {optimal_threshold:.2f}')
plt.legend()
plt.show()



from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf
from sklearn.model_selection import StratifiedKFold

# To store metrics for each fold
accuracies = []
precisions = []
recalls = []
f1_scores = []
roc_aucs = []
specificities = []



for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predictions
    y_pred_prob = resnet50_model.predict(x_val_fold)
    y_pred = np.round(y_pred_prob)

    # Calculate metrics
    accuracies.append(accuracy_score(y_val_fold, y_pred))
    precisions.append(precision_score(y_val_fold, y_pred))
    recalls.append(recall_score(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    roc_aucs.append(roc_auc_score(y_val_fold, y_pred_prob))

    # Specificity calculation
    tn, fp, fn, tp = confusion_matrix(y_val_fold, y_pred).ravel()
    specificities.append(tn / (tn + fp))


# Plot ROC curve and Precision-Recall curve for the last fold as an example
fpr, tpr, _ = roc_curve(y_val_fold, y_pred_prob)
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_aucs[-1]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ResNet50: ROC Curve')
plt.legend(loc='lower right')
plt.show()

precision, recall, _ = precision_recall_curve(y_val_fold, y_pred_prob)
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('ResNet50: Precision-Recall Curve')
plt.show()

# Display Confusion Matrix for the last fold as an example
cm = confusion_matrix(y_val_fold, y_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('ResNet50: Confusion Matrix')
plt.show()

# Display averaged metrics
print(f'Average Accuracy: {np.mean(accuracies)}')
print(f'Average Precision: {np.mean(precisions)}')
print(f'Average Recall: {np.mean(recalls)}')
print(f'Average Specificity: {np.mean(specificities)}')
print(f'Average F1-score: {np.mean(f1_scores)}')
print(f'Average ROC-AUC: {np.mean(roc_aucs)}')

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf
from sklearn.model_selection import StratifiedKFold

n_splits = 5
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# To store metrics and additional data for each fold
accuracies = []
precisions = []
recalls = []
f1_scores = []
roc_aucs = []
specificities = []
all_fpr = []
all_tpr = []
all_precision = []
all_recall = []
conf_matrices = []

for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predictions
    y_pred_prob = resnet50_model.predict(x_val_fold)
    y_pred = np.round(y_pred_prob)

    # Calculate metrics
    accuracies.append(accuracy_score(y_val_fold, y_pred))
    precisions.append(precision_score(y_val_fold, y_pred))
    recalls.append(recall_score(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    roc_aucs.append(roc_auc_score(y_val_fold, y_pred_prob))

    # Specificity calculation
    tn, fp, fn, tp = confusion_matrix(y_val_fold, y_pred).ravel()
    specificities.append(tn / (tn + fp))

    # Store ROC curve data
    fpr, tpr, _ = roc_curve(y_val_fold, y_pred_prob)
    all_fpr.append(fpr)
    all_tpr.append(tpr)

    # Store Precision-Recall curve data
    precision, recall, _ = precision_recall_curve(y_val_fold, y_pred_prob)
    all_precision.append(precision)
    all_recall.append(recall)

    # Store confusion matrix
    conf_matrices.append(confusion_matrix(y_val_fold, y_pred))

# Plot ROC curves for all folds
plt.figure(figsize=(10, 8))
for i in range(n_splits):
    plt.plot(all_fpr[i], all_tpr[i], label=f'Fold {i+1} (AUC = {roc_aucs[i]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ResNet50: ROC Curves Across Folds')
plt.legend(loc='lower right')
plt.show()

# Plot Precision-Recall curves for all folds
plt.figure(figsize=(10, 8))
for i in range(n_splits):
    plt.plot(all_recall[i], all_precision[i], marker='.', label=f'Fold {i+1}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('ResNet50: Precision-Recall Curves Across Folds')
plt.legend()
plt.show()

# Plot confusion matrices for each fold
fig, axes = plt.subplots(1, n_splits, figsize=(20, 4))
for i in range(n_splits):
    sns.heatmap(conf_matrices[i], annot=True, fmt='d', ax=axes[i])
    axes[i].set_xlabel('Predicted')
    axes[i].set_ylabel('Actual')
    axes[i].set_title(f'Fold {i+1} Confusion Matrix')
plt.show()

# Display averaged metrics
print(f'Average Accuracy: {np.mean(accuracies)}')
print(f'Average Precision: {np.mean(precisions)}')
print(f'Average Recall: {np.mean(recalls)}')
print(f'Average Specificity: {np.mean(specificities)}')
print(f'Average F1-score: {np.mean(f1_scores)}')
print(f'Average ROC-AUC: {np.mean(roc_aucs)}')

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import brier_score_loss
from sklearn.metrics import log_loss

# # Initialize StratifiedKFold
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store per-fold metrics
all_ece = []
all_mce = []
all_brier_scores = []
all_log_losses = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predicted probabilities from the model for the validation set
    y_pred_probs = resnet50_model.predict(x_val_fold).flatten()

    # Calculate Brier score and log loss
    brier_score = brier_score_loss(y_val_fold, y_pred_probs)
    log_loss_score = log_loss(y_val_fold, y_pred_probs)
    all_brier_scores.append(brier_score)
    all_log_losses.append(log_loss_score)

    # Calibration curve and ECE/MCE calculations
    n_bins = 10
    bin_edges = np.linspace(0, 1, n_bins + 1)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

    bin_counts = np.histogram(y_pred_probs, bins=bin_edges)[0]
    bin_correct = np.histogram(y_pred_probs[y_val_fold == 1], bins=bin_edges)[0]
    bin_scores = np.histogram(y_pred_probs, bins=bin_edges, weights=y_pred_probs)[0]

    bin_accuracies = np.where(bin_counts > 0, bin_correct / bin_counts, 0)
    bin_confidences = np.where(bin_counts > 0, bin_scores / bin_counts, 0)

    ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_counts) / y_val_fold.size
    mce = np.max(np.abs(bin_accuracies - bin_confidences))

    all_ece.append(ece)
    all_mce.append(mce)

# Average the metrics across all folds
avg_brier_score = np.mean(all_brier_scores)
avg_log_loss = np.mean(all_log_losses)
avg_ece = np.mean(all_ece)
avg_mce = np.mean(all_mce)

print(f'Average Brier Score: {avg_brier_score:.4f}')
print(f'Average Log Loss: {avg_log_loss:.4f}')
print(f'Average ECE: {avg_ece:.4f}')
print(f'Average MCE: {avg_mce:.4f}')

# You may plot the average or last fold's calibration as an example
plt.figure(figsize=(10, 8))
plt.plot(bin_centers, bin_accuracies, 's-', label=f'Model Calibration (ECE={avg_ece:.4f}, MCE={avg_mce:.4f})')
plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')
plt.title('ResNet50: Average Calibration Curve Across Folds')
plt.xlabel('Average Predicted Probability in Each Bin')
plt.ylabel('Fraction of Positives')
plt.legend()
plt.show()



from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import calibration_curve, IsotonicRegression
from sklearn.metrics import brier_score_loss, log_loss
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

# # Initialize StratifiedKFold
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store per-fold metrics
all_metrics = {
    'original': [],
    'platt': [],
    'isotonic': []
}

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]

    # Predict probabilities on the validation fold
    val_probs = resnet50_model.predict(x_val_fold).ravel()

    # Prepare binary arrays for the validation fold
    y_val_binary = np.argmax(y_val_fold, axis=1) if y_val_fold.ndim > 1 else y_val_fold.flatten()

    # Fit Platt Scaling on validation probabilities
    platt_scaler = LogisticRegression(solver='liblinear')
    platt_scaler.fit(val_probs.reshape(-1, 1), y_val_binary)
    calibrated_probs_platt = platt_scaler.predict_proba(val_probs.reshape(-1, 1))[:, 1]

    # Fit and apply Isotonic Regression for calibration on validation probabilities
    isotonic_scaler = IsotonicRegression(out_of_bounds='clip')
    isotonic_scaler.fit(val_probs, y_val_binary)
    calibrated_probs_isotonic = isotonic_scaler.transform(val_probs)

    # Store metrics and probabilities for each fold
    for label, probs in [('original', val_probs), ('platt', calibrated_probs_platt), ('isotonic', calibrated_probs_isotonic)]:
        brier = brier_score_loss(y_val_binary, probs)
        logloss = log_loss(y_val_binary, probs)
        all_metrics[label].append({
            'brier_score': brier,
            'log_loss': logloss,
            'probs': probs
        })

# Average metrics across all folds
for method in all_metrics.keys():
    avg_brier = np.mean([m['brier_score'] for m in all_metrics[method]])
    avg_log_loss = np.mean([m['log_loss'] for m in all_metrics[method]])
    print(f"{method} - Average Brier Score: {avg_brier:.4f}, Average Log Loss: {avg_log_loss:.4f}")

# Plot calibration curves for the test set from the last fold as an example
plt.figure(figsize=(8, 6))
for method in all_metrics.keys():
    prob_true, prob_pred = calibration_curve(y_val_binary, all_metrics[method][-1]['probs'], n_bins=10)
    plt.plot(prob_pred, prob_true, marker='o', linestyle='-', label=method)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')
plt.xlabel('Mean predicted probability')
plt.ylabel('Fraction of positives')
plt.title('ResNet50: Calibration Curves (Validation Set)')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import StratifiedKFold

# # Initialize StratifiedKFold
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store per-fold results
all_cm_original = []
all_cm_platt = []
all_cm_isotonic = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]


    # Generate predicted probabilities and apply calibrations
    val_probs = resnet50_model.predict(x_val_fold).ravel()
    calibrated_probs_platt = platt_scaler.predict_proba(val_probs.reshape(-1, 1))[:, 1]  # Assuming platt_scaler is already fitted in the cross-val setup
    calibrated_probs_isotonic = isotonic_scaler.transform(val_probs)  # Assuming isotonic_scaler is fitted

    # Define a threshold for binary classification
    threshold = 0.25

    # Convert probabilities to binary classifications
    predicted_classes_original = (val_probs >= threshold).astype(int)
    predicted_classes_platt = (calibrated_probs_platt >= threshold).astype(int)
    predicted_classes_isotonic = (calibrated_probs_isotonic >= threshold).astype(int)

    # Compute confusion matrices
    cm_original = confusion_matrix(y_val_fold, predicted_classes_original)
    cm_platt = confusion_matrix(y_val_fold, predicted_classes_platt)
    cm_isotonic = confusion_matrix(y_val_fold, predicted_classes_isotonic)

    # Store confusion matrices
    all_cm_original.append(cm_original)
    all_cm_platt.append(cm_platt)
    all_cm_isotonic.append(cm_isotonic)

    # Optionally print classification reports here or after the loop

# Function to plot confusion matrices
def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(title)
    plt.show()

# Example plotting of the first fold results (or you could average the CM values)
plot_confusion_matrix(all_cm_original[0], 'ResNet50: Confusion Matrix (Original)')
plot_confusion_matrix(all_cm_platt[0], 'ResNet50: Confusion Matrix (Platt Scaling)')
plot_confusion_matrix(all_cm_isotonic[0], 'ResNet50: Confusion Matrix (Isotonic Regression)')

# Example classification report (for the first fold)
print('ResNet50: Classification Report - Original:')
print(classification_report(y_val_fold, predicted_classes_original))
print('ResNet50: Classification Report - Platt Scaling:')
print(classification_report(y_val_fold, predicted_classes_platt))
print('ResNet50: Classification Report - Isotonic Regression:')
print(classification_report(y_val_fold, predicted_classes_isotonic))

def plot_predictions_cross_val(model, dataset, class_names, num_images=28):
    plt.figure(figsize=(14, 10))
    plt.suptitle('ResNet50: Predictions (Cross-Validation)', fontsize=16)

    # Shuffle and take a batch from the dataset
    for images, labels in dataset.shuffle(buffer_size=1000).take(1):  # Shuffle here for randomness
        predictions = model.predict(images)
        predictions = predictions.ravel()  # Flatten the predictions

        # Limit the number of images to plot (in case the last batch has fewer than num_images)
        num_images = min(num_images, images.shape[0])

        # Plot each image in the batch
        for i in range(num_images):
            ax = plt.subplot(4, 7, i + 1)  # Adjust the subplot grid if you change num_images
            plt.imshow(images[i].numpy().astype("uint8"))
            plt.axis('off')

            # Predicted label
            predicted_index = int(predictions[i] >= 0.27)  # Thresholding for binary classification
            true_index = labels[i].numpy()

            # Title with predicted and true labels
            title = f"Pred: {class_names[predicted_index]}\nTrue: {class_names[true_index]}"
            plt.title(title, color='green' if predicted_index == true_index else 'red', fontsize=9)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to make room for the suptitle
    plt.show()


# Define the number of splits for cross-validation
n_splits = 3
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize list to store validation datasets for each fold
val_datasets = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]

    # Convert validation data to TensorFlow Dataset
    val_dataset_fold = tf.data.Dataset.from_tensor_slices((x_val_fold, y_val_fold))
    val_dataset_fold = val_dataset_fold.batch(32)  # Adjust batch size as needed

    # Store the validation dataset for the current fold
    val_datasets.append(val_dataset_fold)

    # Plot predictions for the current fold
    print(f"Fold {len(val_datasets)} Predictions:")
    class_names = ['Benign', 'Malignant']  # Modify as per your classes
    plot_predictions_cross_val(resnet50_model, val_dataset_fold, class_names)

import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
import numpy as np

def plot_predictions_cross_val(images, labels, predictions, class_names, num_images=28):
    plt.figure(figsize=(14, 10))
    plt.suptitle('ResNet50: Predictions (Cross-Validation)', fontsize=16)

    # Ensure not to exceed the array length
    num_images = min(num_images, len(images))

    indices = np.random.choice(range(len(images)), num_images, replace=False)  # Randomly select images to display

    for i, idx in enumerate(indices):
        ax = plt.subplot(4, 7, i + 1)  # Adjust the subplot grid if you change num_images
        plt.imshow(images[idx].astype("uint8"))
        plt.axis('off')

        predicted_index = int(predictions[idx] >= 0.25)  # Threshold for binary classification
        true_index = labels[idx]

        # Title with predicted and true labels
        title = f"Pred: {class_names[predicted_index]}\nTrue: {class_names[true_index]}"
        plt.title(title, color='green' if predicted_index == true_index else 'red', fontsize=9)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to make room for the suptitle
    plt.show()

# Assuming you have a function 'resnet50_model' for predictions and 'X_train', 'Y_train' defined
n_splits = 3
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

all_images = []
all_labels = []
all_predictions = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Predictions for the fold
    predictions = resnet50_model.predict(x_val_fold).ravel()  # Ensure model and method are correctly referenced

    # Store results
    all_images.extend(x_val_fold)
    all_labels.extend(y_val_fold)
    all_predictions.extend(predictions)

# Now plot predictions for all collected data from folds
class_names = ['Benign', 'Malignant']  # Modify as per your classes
plot_predictions_cross_val(np.array(all_images), np.array(all_labels), np.array(all_predictions), class_names)





"""# **Build and Train EfficientNet Model**"""

def weighted_binary_crossentropy(y_true, y_pred, weight_positive, weight_negative):
    y_true_float = tf.cast(y_true, tf.float32)
    bce = tf.keras.losses.binary_crossentropy(y_true_float, y_pred)
    weight_vector = y_true_float * weight_positive + (1. - y_true_float) * weight_negative
    return tf.reduce_mean(weight_vector * bce)

def sensitivity(y_true, y_pred):
    K = tf.keras.backend
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    return true_positives / (possible_positives + K.epsilon())

def specificity(y_true, y_pred):
    K = tf.keras.backend
    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))
    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))
    return true_negatives / (possible_negatives + K.epsilon())

import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense, Conv2D, MaxPooling2D, BatchNormalization, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score

# Build model function with binary classification adjustments
def build_efnetB0_model():
    efnetB0_model = Sequential()
    backbone = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
    efnetB0_model.add(backbone)
    efnetB0_model.add(Conv2D(16, 3, activation="relu", padding="same"))
    efnetB0_model.add(BatchNormalization())
    efnetB0_model.add(MaxPooling2D())
    efnetB0_model.add(Conv2D(32, 3, activation="relu", padding="same"))
    efnetB0_model.add(BatchNormalization())
    efnetB0_model.add(MaxPooling2D())
    efnetB0_model.add(Flatten())
    efnetB0_model.add(Dropout(0.5))
    efnetB0_model.add(Dense(1, activation='sigmoid'))  # Sigmoid activation for binary classification
    return efnetB0_model

# Function to train the model with data augmentation
def train_efnetB0_model(model_function, x_train, y_train, x_val, y_val, epochs, model_save_dir, class_weights):
    # Build and compile the model
    efnetB0_model = model_function()
    efnetB0_model.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss=lambda y_true, y_pred: weighted_binary_crossentropy(y_true, y_pred, weight_positive=class_weights[1], weight_negative=class_weights[0]),
        metrics=['accuracy', sensitivity, specificity, AUC(name='roc_auc')]
    )

    datagen = ImageDataGenerator(
        # preprocessing_function=preprocess_image,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    early_stop = EarlyStopping(monitor='val_loss', patience=5)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)

    train_generator = datagen.flow(x_train, y_train, batch_size=16)
    val_generator = datagen.flow(x_val, y_val, batch_size=16, shuffle=False)

    history = efnetB0_model.fit(
        train_generator,
        steps_per_epoch=len(x_train) // 16,
        epochs=epochs,
        validation_data=val_generator,
        validation_steps=len(x_val) // 16,
        class_weight=class_weights,
        callbacks=[early_stop, reduce_lr]
    )

    efnetB0_model.save(os.path.join(model_save_dir, "efnetB0_model.h5"))
    return efnetB0_model, history

# Define your preprocessed dataset
images = np.array([i[0] for i in full_data])
labels = np.array([i[1] for i in full_data])

# Split the dataset into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Define the number of splits for cross-validation
n_splits = 5
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize list to store validation scores
validation_scores = []

# Create a new folder for cross-validation results
model_save_dir = "/content/drive/MyDrive/KaggleData/CrossValidationResults"
os.makedirs(model_save_dir, exist_ok=True)

# Loop over each fold
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]

    # Class weights for imbalanced data
    total_samples = len(y_train_fold)
    class_counts = np.unique(y_train_fold, return_counts=True)[1]
    weight_for_0 = total_samples / (2 * class_counts[0])
    weight_for_1 = total_samples / (2 * class_counts[1])
    class_weights = {0: weight_for_0, 1: weight_for_1}

    # Train the model
    efnetB0_model, history = train_efnetB0_model(build_efnetB0_model, x_train_fold, y_train_fold, x_val_fold, y_val_fold, 12, model_save_dir, class_weights)

    # Evaluate the model on validation data
    val_loss, val_accuracy, val_sensitivity, val_specificity, val_auc = efnetB0_model.evaluate(x_val_fold, y_val_fold)
    print(f'Validation Loss: {val_loss}')
    print(f'Validation Accuracy: {val_accuracy}')
    print(f'Validation Sensitivity: {val_sensitivity}')
    print(f'Validation Specificity: {val_specificity}')
    print(f'Validation AUC: {val_auc}')

    # Store validation scores
    validation_scores.append((val_loss, val_accuracy, val_sensitivity, val_specificity, val_auc))

# Calculate average validation scores across folds
average_validation_scores = np.mean(validation_scores, axis=0)
print("Average Validation Scores:")
print(f"Loss: {average_validation_scores[0]}")
print(f"Accuracy: {average_validation_scores[1]}")
print(f"Sensitivity: {average_validation_scores[2]}")
print(f"Specificity: {average_validation_scores[3]}")
print(f"AUC: {average_validation_scores[4]}")


# Save model and training history
model_path = os.path.join(model_save_dir, "efnetB0_model.h5")  # Ensure the correct model name here
efnetB0_model.save(model_path)  # Make sure to use the correct variable (efnetB0_model)

history_path = os.path.join(model_save_dir, "efnetB0_model_history.pkl")
with open(history_path, 'wb') as f:
    pickle.dump(history.history, f)  # Ensure using the correct history variable

print(f"EfficientNetB0 Model and training history saved to {model_save_dir}")

#Plotting the model results

#Getting the accuracy
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

#Getting the losses
loss = history.history['loss']
val_loss = history.history['val_loss']

#No of epochs it trained
epochs_range = history.epoch

#Plotting Training and Validation accuracy
plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('EfficientNetB0: Training and Validation Accuracy')

#Plotting Training and Validation Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('ResNeEfficientNetB0: Training and Validation Loss')
plt.show()

import os
import pickle
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.metrics import AUC

# Custom Sensitivity and Specificity Metrics
def sensitivity(y_true, y_pred):
    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))
    possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))
    return true_positives / (possible_positives + tf.keras.backend.epsilon())

def specificity(y_true, y_pred):
    true_negatives = tf.reduce_sum(tf.round(tf.clip_by_value((1-y_true) * (1-y_pred), 0, 1)))
    possible_negatives = tf.reduce_sum(tf.round(tf.clip_by_value(1-y_true, 0, 1)))
    return true_negatives / (possible_negatives + tf.keras.backend.epsilon())

# Custom Weighted Binary Crossentropy
def weighted_binary_crossentropy(y_true, y_pred, weight_positive=1.0, weight_negative=1.0):
    y_true_float = tf.cast(y_true, tf.float32)
    bce = tf.keras.losses.binary_crossentropy(y_true_float, y_pred, from_logits=False)
    weight_vector = y_true_float * weight_positive + (1. - y_true_float) * weight_negative
    weighted_bce = weight_vector * bce
    return tf.reduce_mean(weighted_bce)

# Define the path to the directory where the model and history file are saved
model_save_dir = "/content/drive/MyDrive/KaggleData/CrossValidationResults"
model_path = os.path.join(model_save_dir, "efnetB0_model.h5")


efnetB0_model = load_model(model_path, custom_objects={
    '<lambda>': lambda y_true, y_pred: weighted_binary_crossentropy(y_true, y_pred, weight_positive=1.0, weight_negative=1.0),
    'sensitivity': sensitivity,
    'specificity': specificity,
    'AUC': tf.keras.metrics.AUC
})

# Print the model summary to check its architecture
try:
    efnetB0_model.summary()
except AttributeError as e:
    print("Model is not loaded correctly:", e)

# Load the training history
history_path = os.path.join(model_save_dir, "efnetB0_model_history.pkl")
if os.path.exists(history_path):
    with open(history_path, 'rb') as f:
        history = pickle.load(f)
    print("Training history successfully loaded.")
else:
    print("History file does not exist.")

# # Optionally, check some details from the loaded history
# if 'history' in locals():
#     print("Loaded history keys:", history.keys())
#     print("Example - Accuracy over epochs:", history.get('accuracy'))

import matplotlib.pyplot as plt

# Assume 'history' is the object returned by the fit method of a model, you should use history.history to access the metrics
history_dict = history

# Check if the history_dict contains necessary data
if history_dict and 'accuracy' in history_dict and 'val_accuracy' in history_dict:
    # Extract the number of epochs from the history_dict
    epochs_range = range(len(history_dict['accuracy']))

    # Plotting Training and Validation accuracy
    plt.figure(figsize=(14, 5))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, history_dict['accuracy'], label='Training Accuracy')
    plt.plot(epochs_range, history_dict['val_accuracy'], label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('EfficientNetB0: Training and Validation Accuracy')

    # Plotting Training and Validation loss
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, history_dict['loss'], label='Training Loss')
    plt.plot(epochs_range, history_dict['val_loss'], label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('EfficientNetB0: Training and Validation Loss')

    plt.show()
else:
    print("The necessary training metrics are not available in the loaded history_dict.")



from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
import numpy as np

def calculate_f1_scores(probabilities, true_labels):
    """Calculates F1 scores for a range of thresholds."""
    thresholds = np.arange(0, 1, 0.01)
    f1_scores = [f1_score(true_labels, probabilities > threshold) for threshold in thresholds]
    return thresholds, f1_scores

def find_optimal_threshold(probabilities, true_labels):
    """Finds the threshold that yields the highest F1 score."""
    thresholds, f1_scores = calculate_f1_scores(probabilities, true_labels)
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    optimal_f1_score = f1_scores[optimal_idx]
    return optimal_threshold, optimal_f1_score


predicted_probs = []
actual_labels = []

# Iterate over each fold
for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predicted probabilities from the model for the validation set
    # Reduce the batch size in your predict call
    probs = efnetB0_model.predict(x_val_fold, batch_size=16).flatten()  # Adjust the batch size based on your available GPU memory

    predicted_probs.extend(probs)
    actual_labels.extend(y_val_fold)

# Convert lists to numpy arrays for analysis
predicted_probs = np.array(predicted_probs)
actual_labels = np.array(actual_labels)

# Find the optimal threshold and F1 score

optimal_threshold, optimal_f1_score = find_optimal_threshold(predicted_probs, actual_labels)
print(f"Optimal threshold: {optimal_threshold}")
print(f"Optimal F1 Score: {optimal_f1_score}")

# Plot F1 scores across thresholds
thresholds, f1_scores = calculate_f1_scores(predicted_probs, actual_labels)
plt.plot(thresholds, f1_scores)
plt.xlabel('Threshold')
plt.ylabel('F1 Score')
plt.title('EfficientNetB0: Threshold vs F1 Score')
plt.axvline(x=optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold = {optimal_threshold:.2f}')
plt.legend()
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf
from sklearn.model_selection import StratifiedKFold

# Assume X_train, Y_train are defined and efnetB0_model is loaded

# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# To store metrics for each fold
accuracies = []
precisions = []
recalls = []
f1_scores = []
roc_aucs = []
specificities = []

for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predictions
    y_pred_prob = efnetB0_model.predict(x_val_fold)
    y_pred = np.round(y_pred_prob)

    # Calculate metrics
    accuracies.append(accuracy_score(y_val_fold, y_pred))
    precisions.append(precision_score(y_val_fold, y_pred))
    recalls.append(recall_score(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    roc_aucs.append(roc_auc_score(y_val_fold, y_pred_prob))

    # Specificity calculation
    tn, fp, fn, tp = confusion_matrix(y_val_fold, y_pred).ravel()
    specificities.append(tn / (tn + fp))


# Plot ROC curve and Precision-Recall curve for the last fold as an example
fpr, tpr, _ = roc_curve(y_val_fold, y_pred_prob)
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_aucs[-1]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('EfficientNetB0: ROC Curve')
plt.legend(loc='lower right')
plt.show()

precision, recall, _ = precision_recall_curve(y_val_fold, y_pred_prob)
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('EfficientNetB0: Precision-Recall Curve')
plt.show()

# Display Confusion Matrix for the last fold as an example
cm = confusion_matrix(y_val_fold, y_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('EfficientNetB0: Confusion Matrix')
plt.show()

# Display averaged metrics
print(f'Average Accuracy: {np.mean(accuracies)}')
print(f'Average Precision: {np.mean(precisions)}')
print(f'Average Recall: {np.mean(recalls)}')
print(f'Average Specificity: {np.mean(specificities)}')
print(f'Average F1-score: {np.mean(f1_scores)}')
print(f'Average ROC-AUC: {np.mean(roc_aucs)}')

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf
from sklearn.model_selection import StratifiedKFold

# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# To store metrics and additional data for each fold
accuracies = []
precisions = []
recalls = []
f1_scores = []
roc_aucs = []
specificities = []
all_fpr = []
all_tpr = []
all_precision = []
all_recall = []
conf_matrices = []

for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predictions
    y_pred_prob = efnetB0_model.predict(x_val_fold)
    y_pred = np.round(y_pred_prob)

    # Calculate metrics
    accuracies.append(accuracy_score(y_val_fold, y_pred))
    precisions.append(precision_score(y_val_fold, y_pred))
    recalls.append(recall_score(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    roc_aucs.append(roc_auc_score(y_val_fold, y_pred_prob))

    # Specificity calculation
    tn, fp, fn, tp = confusion_matrix(y_val_fold, y_pred).ravel()
    specificities.append(tn / (tn + fp))

    # Store ROC curve data
    fpr, tpr, _ = roc_curve(y_val_fold, y_pred_prob)
    all_fpr.append(fpr)
    all_tpr.append(tpr)

    # Store Precision-Recall curve data
    precision, recall, _ = precision_recall_curve(y_val_fold, y_pred_prob)
    all_precision.append(precision)
    all_recall.append(recall)

    # Store confusion matrix
    conf_matrices.append(confusion_matrix(y_val_fold, y_pred))

# Plot ROC curves for all folds
plt.figure(figsize=(10, 8))
for i in range(n_splits):
    plt.plot(all_fpr[i], all_tpr[i], label=f'Fold {i+1} (AUC = {roc_aucs[i]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('EfficientNetB0: ROC Curves Across Folds')
plt.legend(loc='lower right')
plt.show()

# Plot Precision-Recall curves for all folds
plt.figure(figsize=(10, 8))
for i in range(n_splits):
    plt.plot(all_recall[i], all_precision[i], marker='.', label=f'Fold {i+1}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('EfficientNet0: Precision-Recall Curves Across Folds')
plt.legend()
plt.show()

# Plot confusion matrices for each fold
fig, axes = plt.subplots(1, n_splits, figsize=(20, 4))
for i in range(n_splits):
    sns.heatmap(conf_matrices[i], annot=True, fmt='d', ax=axes[i])
    axes[i].set_xlabel('Predicted')
    axes[i].set_ylabel('Actual')
    axes[i].set_title(f'Fold {i+1} Confusion Matrix')
plt.show()

# Display averaged metrics
print(f'Average Accuracy: {np.mean(accuracies)}')
print(f'Average Precision: {np.mean(precisions)}')
print(f'Average Recall: {np.mean(recalls)}')
print(f'Average Specificity: {np.mean(specificities)}')
print(f'Average F1-score: {np.mean(f1_scores)}')
print(f'Average ROC-AUC: {np.mean(roc_aucs)}')

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import brier_score_loss
from sklearn.metrics import log_loss

# # Initialize StratifiedKFold
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store per-fold metrics
all_ece = []
all_mce = []
all_brier_scores = []
all_log_losses = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predicted probabilities from the model for the validation set
    y_pred_probs = efnetB0_model.predict(x_val_fold).flatten()

    # Calculate Brier score and log loss
    brier_score = brier_score_loss(y_val_fold, y_pred_probs)
    log_loss_score = log_loss(y_val_fold, y_pred_probs)
    all_brier_scores.append(brier_score)
    all_log_losses.append(log_loss_score)

    # Calibration curve and ECE/MCE calculations
    n_bins = 10
    bin_edges = np.linspace(0, 1, n_bins + 1)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

    bin_counts = np.histogram(y_pred_probs, bins=bin_edges)[0]
    bin_correct = np.histogram(y_pred_probs[y_val_fold == 1], bins=bin_edges)[0]
    bin_scores = np.histogram(y_pred_probs, bins=bin_edges, weights=y_pred_probs)[0]

    bin_accuracies = np.where(bin_counts > 0, bin_correct / bin_counts, 0)
    bin_confidences = np.where(bin_counts > 0, bin_scores / bin_counts, 0)

    ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_counts) / y_val_fold.size
    mce = np.max(np.abs(bin_accuracies - bin_confidences))

    all_ece.append(ece)
    all_mce.append(mce)

# Average the metrics across all folds
avg_brier_score = np.mean(all_brier_scores)
avg_log_loss = np.mean(all_log_losses)
avg_ece = np.mean(all_ece)
avg_mce = np.mean(all_mce)

print(f'Average Brier Score: {avg_brier_score:.4f}')
print(f'Average Log Loss: {avg_log_loss:.4f}')
print(f'Average ECE: {avg_ece:.4f}')
print(f'Average MCE: {avg_mce:.4f}')

# You may plot the average or last fold's calibration as an example
plt.figure(figsize=(10, 8))
plt.plot(bin_centers, bin_accuracies, 's-', label=f'Model Calibration (ECE={avg_ece:.4f}, MCE={avg_mce:.4f})')
plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')
plt.title('EfficientNetB0: Average Calibration Curve Across Folds')
plt.xlabel('Average Predicted Probability in Each Bin')
plt.ylabel('Fraction of Positives')
plt.legend()
plt.show()

from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import calibration_curve, IsotonicRegression
from sklearn.metrics import brier_score_loss, log_loss
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

# # Initialize StratifiedKFold
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store per-fold metrics
all_metrics = {
    'original': [],
    'platt': [],
    'isotonic': []
}

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]

    # Predict probabilities on the validation fold
    val_probs = efnetB0_model.predict(x_val_fold).ravel()

    # Prepare binary arrays for the validation fold
    y_val_binary = np.argmax(y_val_fold, axis=1) if y_val_fold.ndim > 1 else y_val_fold.flatten()

    # Fit Platt Scaling on validation probabilities
    platt_scaler = LogisticRegression(solver='liblinear')
    platt_scaler.fit(val_probs.reshape(-1, 1), y_val_binary)
    calibrated_probs_platt = platt_scaler.predict_proba(val_probs.reshape(-1, 1))[:, 1]

    # Fit and apply Isotonic Regression for calibration on validation probabilities
    isotonic_scaler = IsotonicRegression(out_of_bounds='clip')
    isotonic_scaler.fit(val_probs, y_val_binary)
    calibrated_probs_isotonic = isotonic_scaler.transform(val_probs)

    # Store metrics and probabilities for each fold
    for label, probs in [('original', val_probs), ('platt', calibrated_probs_platt), ('isotonic', calibrated_probs_isotonic)]:
        brier = brier_score_loss(y_val_binary, probs)
        logloss = log_loss(y_val_binary, probs)
        all_metrics[label].append({
            'brier_score': brier,
            'log_loss': logloss,
            'probs': probs
        })

# Average metrics across all folds
for method in all_metrics.keys():
    avg_brier = np.mean([m['brier_score'] for m in all_metrics[method]])
    avg_log_loss = np.mean([m['log_loss'] for m in all_metrics[method]])
    print(f"{method} - Average Brier Score: {avg_brier:.4f}, Average Log Loss: {avg_log_loss:.4f}")

# Plot calibration curves for the test set from the last fold as an example
plt.figure(figsize=(8, 6))
for method in all_metrics.keys():
    prob_true, prob_pred = calibration_curve(y_val_binary, all_metrics[method][-1]['probs'], n_bins=10)
    plt.plot(prob_pred, prob_true, marker='o', linestyle='-', label=method)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')
plt.xlabel('Mean predicted probability')
plt.ylabel('Fraction of positives')
plt.title('EfficientNetB0: Calibration Curves (Validation Set)')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import StratifiedKFold

# # Initialize StratifiedKFold
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store per-fold results
all_cm_original = []
all_cm_platt = []
all_cm_isotonic = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]


    # Generate predicted probabilities and apply calibrations
    val_probs = efnetB0_model.predict(x_val_fold).ravel()
    calibrated_probs_platt = platt_scaler.predict_proba(val_probs.reshape(-1, 1))[:, 1]  # Assuming platt_scaler is already fitted in the cross-val setup
    calibrated_probs_isotonic = isotonic_scaler.transform(val_probs)  # Assuming isotonic_scaler is fitted

    # Define a threshold for binary classification
    threshold = 0.59

    # Convert probabilities to binary classifications
    predicted_classes_original = (val_probs >= threshold).astype(int)
    predicted_classes_platt = (calibrated_probs_platt >= threshold).astype(int)
    predicted_classes_isotonic = (calibrated_probs_isotonic >= threshold).astype(int)

    # Compute confusion matrices
    cm_original = confusion_matrix(y_val_fold, predicted_classes_original)
    cm_platt = confusion_matrix(y_val_fold, predicted_classes_platt)
    cm_isotonic = confusion_matrix(y_val_fold, predicted_classes_isotonic)

    # Store confusion matrices
    all_cm_original.append(cm_original)
    all_cm_platt.append(cm_platt)
    all_cm_isotonic.append(cm_isotonic)

    # Optionally print classification reports here or after the loop

# Function to plot confusion matrices
def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(title)
    plt.show()

# Example plotting of the first fold results (or you could average the CM values)
plot_confusion_matrix(all_cm_original[0], 'EfficientNetB0: Confusion Matrix (Original)')
plot_confusion_matrix(all_cm_platt[0], 'EfficientNetB0: Confusion Matrix (Platt Scaling)')
plot_confusion_matrix(all_cm_isotonic[0], 'EfficientNetB0: Confusion Matrix (Isotonic Regression)')

# Example classification report (for the first fold)
print('EfficientNetB0: Classification Report - Original:')
print(classification_report(y_val_fold, predicted_classes_original))
print('EfficientNetB0: Classification Report - Platt Scaling:')
print(classification_report(y_val_fold, predicted_classes_platt))
print('EfficientNetB0: Classification Report - Isotonic Regression:')
print(classification_report(y_val_fold, predicted_classes_isotonic))

def plot_predictions_cross_val(model, dataset, class_names, num_images=28):
    plt.figure(figsize=(14, 10))
    plt.suptitle('EfficientNetB0: Model Predictions (Cross-Validation)', fontsize=16)

    # Shuffle and take a batch from the dataset
    for images, labels in dataset.shuffle(buffer_size=1000).take(1):  # Shuffle here for randomness
        predictions = model.predict(images)
        predictions = predictions.ravel()  # Flatten the predictions

        # Limit the number of images to plot (in case the last batch has fewer than num_images)
        num_images = min(num_images, images.shape[0])

        # Plot each image in the batch
        for i in range(num_images):
            ax = plt.subplot(4, 7, i + 1)  # Adjust the subplot grid if you change num_images
            plt.imshow(images[i].numpy().astype("uint8"))
            plt.axis('off')

            # Predicted label
            predicted_index = int(predictions[i] >= 0.59)  # Thresholding for binary classification
            true_index = labels[i].numpy()

            # Title with predicted and true labels
            title = f"Pred: {class_names[predicted_index]}\nTrue: {class_names[true_index]}"
            plt.title(title, color='green' if predicted_index == true_index else 'red', fontsize=9)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to make room for the suptitle
    plt.show()


# Define the number of splits for cross-validation
n_splits = 5
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize list to store validation datasets for each fold
val_datasets = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]

    # Convert validation data to TensorFlow Dataset
    val_dataset_fold = tf.data.Dataset.from_tensor_slices((x_val_fold, y_val_fold))
    val_dataset_fold = val_dataset_fold.batch(32)  # Adjust batch size as needed

    # Store the validation dataset for the current fold
    val_datasets.append(val_dataset_fold)

    # Plot predictions for the current fold
    print(f"Fold {len(val_datasets)} Predictions:")
    class_names = ['Benign', 'Malignant']  # Modify as per your classes
    plot_predictions_cross_val(efnetB0_model, val_dataset_fold, class_names)

import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
import numpy as np

def plot_predictions_cross_val(images, labels, predictions, class_names, num_images=28):
    plt.figure(figsize=(14, 10))
    plt.suptitle('EfficientNetB0: Model Predictions (Cross-Validation)', fontsize=16)

    # Ensure not to exceed the array length
    num_images = min(num_images, len(images))

    indices = np.random.choice(range(len(images)), num_images, replace=False)  # Randomly select images to display

    for i, idx in enumerate(indices):
        ax = plt.subplot(4, 7, i + 1)  # Adjust the subplot grid if you change num_images
        plt.imshow(images[idx].astype("uint8"))
        plt.axis('off')

        predicted_index = int(predictions[idx] >= 0.59)  # Threshold for binary classification
        true_index = labels[idx]

        # Title with predicted and true labels
        title = f"Pred: {class_names[predicted_index]}\nTrue: {class_names[true_index]}"
        plt.title(title, color='green' if predicted_index == true_index else 'red', fontsize=9)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to make room for the suptitle
    plt.show()

# Assuming you have a function 'resnet50_model' for predictions and 'X_train', 'Y_train' defined
n_splits = 3
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

all_images = []
all_labels = []
all_predictions = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Predictions for the fold
    predictions = efnetB0_model.predict(x_val_fold).ravel()  # Ensure model and method are correctly referenced

    # Store results
    all_images.extend(x_val_fold)
    all_labels.extend(y_val_fold)
    all_predictions.extend(predictions)

# Now plot predictions for all collected data from folds
class_names = ['Benign', 'Malignant']  # Modify as per your classes
plot_predictions_cross_val(np.array(all_images), np.array(all_labels), np.array(all_predictions), class_names)

"""# **DenseNet121**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import DenseNet121

from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense, Conv2D, MaxPooling2D, BatchNormalization, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score


# Sensitivity and Specificity metrics
def sensitivity(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    return true_positives / (possible_positives + K.epsilon())

def specificity(y_true, y_pred):
    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))
    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))
    return true_negatives / (possible_negatives + K.epsilon())

# Custom weighted binary cross-entropy
def weighted_binary_crossentropy(y_true, y_pred, weight_positive=1.0, weight_negative=1.0):
    y_true_float = tf.cast(y_true, tf.float32)
    bce = tf.keras.losses.binary_crossentropy(y_true_float, y_pred, from_logits=False)
    weight_vector = y_true_float * weight_positive + (1. - y_true_float) * weight_negative
    weighted_bce = weight_vector * bce
    return tf.reduce_mean(weighted_bce)

import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import DenseNet121

from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense, Conv2D, MaxPooling2D, BatchNormalization, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score


# Build model function with binary classification adjustments
# Define the function to build the DenseNet121 model
def build_densenet121_model():
    densenet121_model = Sequential()
    backbone = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    densenet121_model.add(backbone)
    densenet121_model.add(Conv2D(16, 3, activation="relu", padding="same"))
    densenet121_model.add(BatchNormalization())
    densenet121_model.add(MaxPooling2D())
    densenet121_model.add(Conv2D(32, 3, activation="relu", padding="same"))
    densenet121_model.add(BatchNormalization())
    densenet121_model.add(MaxPooling2D())
    densenet121_model.add(Flatten())
    densenet121_model.add(Dropout(0.5))
    densenet121_model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification
    return densenet121_model

# Function to train the model with data augmentation
def train_densenet121_model(model_function, x_train, y_train, x_val, y_val, epochs, model_save_dir, class_weights):
    # Build and compile the model
    densenet121_model = model_function()
    densenet121_model.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss=lambda y_true, y_pred: weighted_binary_crossentropy(y_true, y_pred, weight_positive=class_weights[1], weight_negative=class_weights[0]),
        metrics=['accuracy', sensitivity, specificity, AUC(name='roc_auc')]
    )

    # Define callbacks
    early_stop = EarlyStopping(monitor='val_loss', patience=5)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)

    datagen = ImageDataGenerator(
        # preprocessing_function=preprocess_image,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    train_generator = datagen.flow(x_train, y_train, batch_size=16)
    val_generator = datagen.flow(x_val, y_val, batch_size=16, shuffle=False)

    history = densenet121_model.fit(
        train_generator,
        steps_per_epoch=len(x_train) // 16,
        epochs=epochs,
        validation_data=val_generator,
        validation_steps=len(x_val) // 16,
        class_weight=class_weights,
        callbacks=[early_stop, reduce_lr]
    )

    densenet121_model.save(os.path.join(model_save_dir, "efnetB0_model.h5"))
    return densenet121_model, history

# Define your preprocessed dataset
images = np.array([i[0] for i in full_data])
labels = np.array([i[1] for i in full_data])

# Split the dataset into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Define the number of splits for cross-validation
n_splits = 5
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize list to store validation scores
validation_scores = []

# Create a new folder for cross-validation results
model_save_dir = "/content/drive/MyDrive/KaggleData/CrossValidationResults"
os.makedirs(model_save_dir, exist_ok=True)

# Loop over each fold
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]

    # Class weights for imbalanced data
    total_samples = len(y_train_fold)
    class_counts = np.unique(y_train_fold, return_counts=True)[1]
    weight_for_0 = total_samples / (2 * class_counts[0])
    weight_for_1 = total_samples / (2 * class_counts[1])
    class_weights = {0: weight_for_0, 1: weight_for_1}

    # Train the model
    densenet121_model, history = train_densenet121_model(build_densenet121_model, x_train_fold, y_train_fold, x_val_fold, y_val_fold, 12, model_save_dir, class_weights)

    # Evaluate the model on validation data
    val_loss, val_accuracy, val_sensitivity, val_specificity, val_auc = densenet121_model.evaluate(x_val_fold, y_val_fold)
    print(f'Validation Loss: {val_loss}')
    print(f'Validation Accuracy: {val_accuracy}')
    print(f'Validation Sensitivity: {val_sensitivity}')
    print(f'Validation Specificity: {val_specificity}')
    print(f'Validation AUC: {val_auc}')

    # Store validation scores
    validation_scores.append((val_loss, val_accuracy, val_sensitivity, val_specificity, val_auc))

# Calculate average validation scores across folds
average_validation_scores = np.mean(validation_scores, axis=0)
print("Average Validation Scores:")
print(f"Loss: {average_validation_scores[0]}")
print(f"Accuracy: {average_validation_scores[1]}")
print(f"Sensitivity: {average_validation_scores[2]}")
print(f"Specificity: {average_validation_scores[3]}")
print(f"AUC: {average_validation_scores[4]}")


# Save model and training history
model_path = os.path.join(model_save_dir, "densenet121_model.h5")  # Ensure the correct model name here
densenet121_model.save(model_path)  # Make sure to use the correct variable (densenet121_model)

history_path = os.path.join(model_save_dir, "densenet121_model_history.pkl")
with open(history_path, 'wb') as f:
    pickle.dump(history.history, f)  # Ensure using the correct history variable

print(f"DenseNet121 Model and training history saved to {model_save_dir}")

# import os
# import pickle

# # Create a new folder for cross-validation results
# model_save_dir = "/content/drive/MyDrive/KaggleData/XVal"
# os.makedirs(model_save_dir, exist_ok=True)

# # Save model and training history
# model_path = os.path.join(model_save_dir, "densenet121.h5")  # Ensure the correct model name here
# densenet121_model.save(model_path)  # Make sure to use the correct variable (densenet121_model)

# history_path = os.path.join(model_save_dir, "densenet121_history.pkl")
# with open(history_path, 'wb') as f:
#     pickle.dump(history.history, f)  # Ensure using the correct history variable

# print(f"densenet121 Model and training history saved to {model_save_dir}")



#Plotting the model results

#Getting the accuracy
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

#Getting the losses
loss = history.history['loss']
val_loss = history.history['val_loss']

#No of epochs it trained
epochs_range = history.epoch

#Plotting Training and Validation accuracy
plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('DenseNet121: Training and Validation Accuracy')

#Plotting Training and Validation Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('DenseNet121: Training and Validation Loss')
plt.show()



from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
import numpy as np

def calculate_f1_scores(probabilities, true_labels):
    """Calculates F1 scores for a range of thresholds."""
    thresholds = np.arange(0, 1, 0.01)
    f1_scores = [f1_score(true_labels, probabilities > threshold) for threshold in thresholds]
    return thresholds, f1_scores

def find_optimal_threshold(probabilities, true_labels):
    """Finds the threshold that yields the highest F1 score."""
    thresholds, f1_scores = calculate_f1_scores(probabilities, true_labels)
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    optimal_f1_score = f1_scores[optimal_idx]
    return optimal_threshold, optimal_f1_score

# # Extract images and labels
# images = np.array([i[0] for i in full_data])
# labels = np.array([i[1] for i in full_data])

# # Split the dataset into training and testing sets
# X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# # Assuming X_train and Y_train are already defined and preprocessed appropriately
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

predicted_probs = []
actual_labels = []

# Iterate over each fold
for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predicted probabilities from the model for the validation set
    # Reduce the batch size in your predict call
    probs = densenet121_model.predict(x_val_fold, batch_size=16).flatten()  # Adjust the batch size based on your available GPU memory

    predicted_probs.extend(probs)
    actual_labels.extend(y_val_fold)

# Convert lists to numpy arrays for analysis
predicted_probs = np.array(predicted_probs)
actual_labels = np.array(actual_labels)

# Find the optimal threshold and F1 score

optimal_threshold, optimal_f1_score = find_optimal_threshold(predicted_probs, actual_labels)
print(f"Optimal threshold: {optimal_threshold}")
print(f"Optimal F1 Score: {optimal_f1_score}")

# Plot F1 scores across thresholds
thresholds, f1_scores = calculate_f1_scores(predicted_probs, actual_labels)
plt.plot(thresholds, f1_scores)
plt.xlabel('Threshold')
plt.ylabel('F1 Score')
plt.title('Densenet121: Threshold vs F1 Score')
plt.axvline(x=optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold = {optimal_threshold:.2f}')
plt.legend()
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf
from sklearn.model_selection import StratifiedKFold

# # Assume X_train, Y_train are defined and resnet50_model is loaded

# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# To store metrics for each fold
accuracies = []
precisions = []
recalls = []
f1_scores = []
roc_aucs = []
specificities = []

for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predictions
    y_pred_prob = densenet121_model.predict(x_val_fold)
    y_pred = np.round(y_pred_prob)

    # Calculate metrics
    accuracies.append(accuracy_score(y_val_fold, y_pred))
    precisions.append(precision_score(y_val_fold, y_pred))
    recalls.append(recall_score(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    roc_aucs.append(roc_auc_score(y_val_fold, y_pred_prob))

    # Specificity calculation
    tn, fp, fn, tp = confusion_matrix(y_val_fold, y_pred).ravel()
    specificities.append(tn / (tn + fp))


# Plot ROC curve and Precision-Recall curve for the last fold as an example
fpr, tpr, _ = roc_curve(y_val_fold, y_pred_prob)
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_aucs[-1]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('DenseNet121: ROC Curve')
plt.legend(loc='lower right')
plt.show()

precision, recall, _ = precision_recall_curve(y_val_fold, y_pred_prob)
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('DenseNet121: Precision-Recall Curve')
plt.show()

# Display Confusion Matrix for the last fold as an example
cm = confusion_matrix(y_val_fold, y_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('DenseNet121: Confusion Matrix')
plt.show()

# Display averaged metrics
print(f'Average Accuracy: {np.mean(accuracies)}')
print(f'Average Precision: {np.mean(precisions)}')
print(f'Average Recall: {np.mean(recalls)}')
print(f'Average Specificity: {np.mean(specificities)}')
print(f'Average F1-score: {np.mean(f1_scores)}')
print(f'Average ROC-AUC: {np.mean(roc_aucs)}')

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf
from sklearn.model_selection import StratifiedKFold

# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# To store metrics and additional data for each fold
accuracies = []
precisions = []
recalls = []
f1_scores = []
roc_aucs = []
specificities = []
all_fpr = []
all_tpr = []
all_precision = []
all_recall = []
conf_matrices = []

for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predictions
    y_pred_prob = densenet121_model.predict(x_val_fold)
    y_pred = np.round(y_pred_prob)

    # Calculate metrics
    accuracies.append(accuracy_score(y_val_fold, y_pred))
    precisions.append(precision_score(y_val_fold, y_pred))
    recalls.append(recall_score(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    roc_aucs.append(roc_auc_score(y_val_fold, y_pred_prob))

    # Specificity calculation
    tn, fp, fn, tp = confusion_matrix(y_val_fold, y_pred).ravel()
    specificities.append(tn / (tn + fp))

    # Store ROC curve data
    fpr, tpr, _ = roc_curve(y_val_fold, y_pred_prob)
    all_fpr.append(fpr)
    all_tpr.append(tpr)

    # Store Precision-Recall curve data
    precision, recall, _ = precision_recall_curve(y_val_fold, y_pred_prob)
    all_precision.append(precision)
    all_recall.append(recall)

    # Store confusion matrix
    conf_matrices.append(confusion_matrix(y_val_fold, y_pred))

# Plot ROC curves for all folds
plt.figure(figsize=(10, 8))
for i in range(n_splits):
    plt.plot(all_fpr[i], all_tpr[i], label=f'Fold {i+1} (AUC = {roc_aucs[i]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('DenseNet121: ROC Curves Across Folds')
plt.legend(loc='lower right')
plt.show()

# Plot Precision-Recall curves for all folds
plt.figure(figsize=(10, 8))
for i in range(n_splits):
    plt.plot(all_recall[i], all_precision[i], marker='.', label=f'Fold {i+1}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('DenseNet121: Precision-Recall Curves Across Folds')
plt.legend()
plt.show()

# Plot confusion matrices for each fold
fig, axes = plt.subplots(1, n_splits, figsize=(20, 4))
for i in range(n_splits):
    sns.heatmap(conf_matrices[i], annot=True, fmt='d', ax=axes[i])
    axes[i].set_xlabel('Predicted')
    axes[i].set_ylabel('Actual')
    axes[i].set_title(f'Fold {i+1} Confusion Matrix')
plt.show()

# Display averaged metrics
print(f'Average Accuracy: {np.mean(accuracies)}')
print(f'Average Precision: {np.mean(precisions)}')
print(f'Average Recall: {np.mean(recalls)}')
print(f'Average Specificity: {np.mean(specificities)}')
print(f'Average F1-score: {np.mean(f1_scores)}')
print(f'Average ROC-AUC: {np.mean(roc_aucs)}')

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import brier_score_loss
from sklearn.metrics import log_loss

# # Initialize StratifiedKFold
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store per-fold metrics
all_ece = []
all_mce = []
all_brier_scores = []
all_log_losses = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predicted probabilities from the model for the validation set
    y_pred_probs = densenet121_model.predict(x_val_fold).flatten()

    # Calculate Brier score and log loss
    brier_score = brier_score_loss(y_val_fold, y_pred_probs)
    log_loss_score = log_loss(y_val_fold, y_pred_probs)
    all_brier_scores.append(brier_score)
    all_log_losses.append(log_loss_score)

    # Calibration curve and ECE/MCE calculations
    n_bins = 10
    bin_edges = np.linspace(0, 1, n_bins + 1)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

    bin_counts = np.histogram(y_pred_probs, bins=bin_edges)[0]
    bin_correct = np.histogram(y_pred_probs[y_val_fold == 1], bins=bin_edges)[0]
    bin_scores = np.histogram(y_pred_probs, bins=bin_edges, weights=y_pred_probs)[0]

    bin_accuracies = np.where(bin_counts > 0, bin_correct / bin_counts, 0)
    bin_confidences = np.where(bin_counts > 0, bin_scores / bin_counts, 0)

    ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_counts) / y_val_fold.size
    mce = np.max(np.abs(bin_accuracies - bin_confidences))

    all_ece.append(ece)
    all_mce.append(mce)

# Average the metrics across all folds
avg_brier_score = np.mean(all_brier_scores)
avg_log_loss = np.mean(all_log_losses)
avg_ece = np.mean(all_ece)
avg_mce = np.mean(all_mce)

print(f'Average Brier Score: {avg_brier_score:.4f}')
print(f'Average Log Loss: {avg_log_loss:.4f}')
print(f'Average ECE: {avg_ece:.4f}')
print(f'Average MCE: {avg_mce:.4f}')

# You may plot the average or last fold's calibration as an example
plt.figure(figsize=(10, 8))
plt.plot(bin_centers, bin_accuracies, 's-', label=f'Model Calibration (ECE={avg_ece:.4f}, MCE={avg_mce:.4f})')
plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')
plt.title('Densenet121: Average Calibration Curve Across Folds')
plt.xlabel('Average Predicted Probability in Each Bin')
plt.ylabel('Fraction of Positives')
plt.legend()
plt.show()

from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import calibration_curve, IsotonicRegression
from sklearn.metrics import brier_score_loss, log_loss
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

# # Initialize StratifiedKFold
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store per-fold metrics
all_metrics = {
    'original': [],
    'platt': [],
    'isotonic': []
}

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]

    # Predict probabilities on the validation fold
    val_probs = densenet121_model.predict(x_val_fold).ravel()

    # Prepare binary arrays for the validation fold
    y_val_binary = np.argmax(y_val_fold, axis=1) if y_val_fold.ndim > 1 else y_val_fold.flatten()

    # Fit Platt Scaling on validation probabilities
    platt_scaler = LogisticRegression(solver='liblinear')
    platt_scaler.fit(val_probs.reshape(-1, 1), y_val_binary)
    calibrated_probs_platt = platt_scaler.predict_proba(val_probs.reshape(-1, 1))[:, 1]

    # Fit and apply Isotonic Regression for calibration on validation probabilities
    isotonic_scaler = IsotonicRegression(out_of_bounds='clip')
    isotonic_scaler.fit(val_probs, y_val_binary)
    calibrated_probs_isotonic = isotonic_scaler.transform(val_probs)

    # Store metrics and probabilities for each fold
    for label, probs in [('original', val_probs), ('platt', calibrated_probs_platt), ('isotonic', calibrated_probs_isotonic)]:
        brier = brier_score_loss(y_val_binary, probs)
        logloss = log_loss(y_val_binary, probs)
        all_metrics[label].append({
            'brier_score': brier,
            'log_loss': logloss,
            'probs': probs
        })

# Average metrics across all folds
for method in all_metrics.keys():
    avg_brier = np.mean([m['brier_score'] for m in all_metrics[method]])
    avg_log_loss = np.mean([m['log_loss'] for m in all_metrics[method]])
    print(f"{method} - Average Brier Score: {avg_brier:.4f}, Average Log Loss: {avg_log_loss:.4f}")

# Plot calibration curves for the test set from the last fold as an example
plt.figure(figsize=(8, 6))
for method in all_metrics.keys():
    prob_true, prob_pred = calibration_curve(y_val_binary, all_metrics[method][-1]['probs'], n_bins=10)
    plt.plot(prob_pred, prob_true, marker='o', linestyle='-', label=method)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')
plt.xlabel('Mean predicted probability')
plt.ylabel('Fraction of positives')
plt.title('DenseNet121: Calibration Curves (Validation Set)')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import StratifiedKFold

# # Initialize StratifiedKFold
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store per-fold results
all_cm_original = []
all_cm_platt = []
all_cm_isotonic = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]


    # Generate predicted probabilities and apply calibrations
    val_probs = densenet121_model.predict(x_val_fold).ravel()
    calibrated_probs_platt = platt_scaler.predict_proba(val_probs.reshape(-1, 1))[:, 1]  # Assuming platt_scaler is already fitted in the cross-val setup
    calibrated_probs_isotonic = isotonic_scaler.transform(val_probs)  # Assuming isotonic_scaler is fitted

    # Define a threshold for binary classification
    threshold = 0.63

    # Convert probabilities to binary classifications
    predicted_classes_original = (val_probs >= threshold).astype(int)
    predicted_classes_platt = (calibrated_probs_platt >= threshold).astype(int)
    predicted_classes_isotonic = (calibrated_probs_isotonic >= threshold).astype(int)

    # Compute confusion matrices
    cm_original = confusion_matrix(y_val_fold, predicted_classes_original)
    cm_platt = confusion_matrix(y_val_fold, predicted_classes_platt)
    cm_isotonic = confusion_matrix(y_val_fold, predicted_classes_isotonic)

    # Store confusion matrices
    all_cm_original.append(cm_original)
    all_cm_platt.append(cm_platt)
    all_cm_isotonic.append(cm_isotonic)

    # Optionally print classification reports here or after the loop

# Function to plot confusion matrices
def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(title)
    plt.show()

# Example plotting of the first fold results (or you could average the CM values)
plot_confusion_matrix(all_cm_original[0], 'DenseNet121: Confusion Matrix (Original)')
plot_confusion_matrix(all_cm_platt[0], 'DenseNet121: Confusion Matrix (Platt Scaling)')
plot_confusion_matrix(all_cm_isotonic[0], 'DenseNet121: Confusion Matrix (Isotonic Regression)')

# Example classification report (for the first fold)
print('DenseNet121: Classification Report - Original:')
print(classification_report(y_val_fold, predicted_classes_original))
print('DenseNet121: Classification Report - Platt Scaling:')
print(classification_report(y_val_fold, predicted_classes_platt))
print('DenseNet121: Classification Report - Isotonic Regression:')
print(classification_report(y_val_fold, predicted_classes_isotonic))

def plot_predictions_cross_val(model, dataset, class_names, num_images=28):
    plt.figure(figsize=(14, 10))
    plt.suptitle('DenseNet121: Model Predictions (Cross-Validation)', fontsize=16)

    # Shuffle and take a batch from the dataset
    for images, labels in dataset.shuffle(buffer_size=1000).take(1):  # Shuffle here for randomness
        predictions = model.predict(images)
        predictions = predictions.ravel()  # Flatten the predictions

        # Limit the number of images to plot (in case the last batch has fewer than num_images)
        num_images = min(num_images, images.shape[0])

        # Plot each image in the batch
        for i in range(num_images):
            ax = plt.subplot(4, 7, i + 1)  # Adjust the subplot grid if you change num_images
            plt.imshow(images[i].numpy().astype("uint8"))
            plt.axis('off')

            # Predicted label
            predicted_index = int(predictions[i] >= 0.63)  # Thresholding for binary classification
            true_index = labels[i].numpy()

            # Title with predicted and true labels
            title = f"Pred: {class_names[predicted_index]}\nTrue: {class_names[true_index]}"
            plt.title(title, color='green' if predicted_index == true_index else 'red', fontsize=9)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to make room for the suptitle
    plt.show()


# Define the number of splits for cross-validation
n_splits = 5
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize list to store validation datasets for each fold
val_datasets = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]

    # Convert validation data to TensorFlow Dataset
    val_dataset_fold = tf.data.Dataset.from_tensor_slices((x_val_fold, y_val_fold))
    val_dataset_fold = val_dataset_fold.batch(32)  # Adjust batch size as needed

    # Store the validation dataset for the current fold
    val_datasets.append(val_dataset_fold)

    # Plot predictions for the current fold
    print(f"Fold {len(val_datasets)} Predictions:")
    class_names = ['Benign', 'Malignant']  # Modify as per your classes
    plot_predictions_cross_val(densenet121_model, val_dataset_fold, class_names)

import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
import numpy as np

def plot_predictions_cross_val(images, labels, predictions, class_names, num_images=28):
    plt.figure(figsize=(14, 10))
    plt.suptitle('DenseNet121: Model Predictions (Cross-Validation)', fontsize=16)

    # Ensure not to exceed the array length
    num_images = min(num_images, len(images))

    indices = np.random.choice(range(len(images)), num_images, replace=False)  # Randomly select images to display

    for i, idx in enumerate(indices):
        ax = plt.subplot(4, 7, i + 1)  # Adjust the subplot grid if you change num_images
        plt.imshow(images[idx].astype("uint8"))
        plt.axis('off')

        predicted_index = int(predictions[idx] >= 0.59)  # Threshold for binary classification
        true_index = labels[idx]

        # Title with predicted and true labels
        title = f"Pred: {class_names[predicted_index]}\nTrue: {class_names[true_index]}"
        plt.title(title, color='green' if predicted_index == true_index else 'red', fontsize=9)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to make room for the suptitle
    plt.show()

# Assuming you have a function 'densenet121_model' for predictions and 'X_train', 'Y_train' defined
n_splits = 5
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

all_images = []
all_labels = []
all_predictions = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Predictions for the fold
    predictions = densenet121_model.predict(x_val_fold).ravel()  # Ensure model and method are correctly referenced

    # Store results
    all_images.extend(x_val_fold)
    all_labels.extend(y_val_fold)
    all_predictions.extend(predictions)

# Now plot predictions for all collected data from folds
class_names = ['Benign', 'Malignant']  # Modify as per your classes
plot_predictions_cross_val(np.array(all_images), np.array(all_labels), np.array(all_predictions), class_names)

"""# **DenseNet201**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import DenseNet201

from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense, Conv2D, MaxPooling2D, BatchNormalization, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score

# Build model function with binary classification adjustments
def build_densenet201_model():
    densenet201_model = Sequential()
    backbone = DenseNet201(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    densenet201_model.add(backbone)
    densenet201_model.add(Conv2D(16, 3, activation="relu", padding="same"))
    densenet201_model.add(BatchNormalization())
    densenet201_model.add(MaxPooling2D())
    densenet201_model.add(Conv2D(32, 3, activation="relu", padding="same"))
    densenet201_model.add(BatchNormalization())
    densenet201_model.add(MaxPooling2D())
    densenet201_model.add(Flatten())
    densenet201_model.add(Dropout(0.5))
    densenet201_model.add(Dense(1, activation='sigmoid'))  # Sigmoid activation for binary classification
    return densenet201_model

# Train the model
# Function to train the model with data augmentation
def train_densenet201_model(model_function, x_train, y_train, x_val, y_val, epochs, model_save_dir, class_weights):
    # Build and compile the model
    densenet201_model = model_function()
    densenet201_model.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss=lambda y_true, y_pred: weighted_binary_crossentropy(y_true, y_pred, weight_positive=class_weights[1], weight_negative=class_weights[0]),
        metrics=['accuracy', sensitivity, specificity, AUC(name='roc_auc')]
    )

    # Define callbacks
    early_stop = EarlyStopping(monitor='val_loss', patience=5)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)

    datagen = ImageDataGenerator(
        # preprocessing_function=preprocess_image,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    train_generator = datagen.flow(x_train, y_train, batch_size=16)
    val_generator = datagen.flow(x_val, y_val, batch_size=16, shuffle=False)

    history = densenet201_model.fit(
        train_generator,
        steps_per_epoch=len(x_train) // 16,
        epochs=epochs,
        validation_data=val_generator,
        validation_steps=len(x_val) // 16,
        class_weight=class_weights,
        callbacks=[early_stop, reduce_lr]
    )

    densenet201_model.save(os.path.join(model_save_dir, "densenet201_model.h5"))
    return densenet201_model, history

# Define your preprocessed dataset
images = np.array([i[0] for i in full_data])
labels = np.array([i[1] for i in full_data])

# Split the dataset into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Define the number of splits for cross-validation
n_splits = 5
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize list to store validation scores
validation_scores = []

# Create a new folder for cross-validation results
model_save_dir = "/content/drive/MyDrive/KaggleData/CrossValidationResults"
os.makedirs(model_save_dir, exist_ok=True)

# Loop over each fold
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]

    # Class weights for imbalanced data
    total_samples = len(y_train_fold)
    class_counts = np.unique(y_train_fold, return_counts=True)[1]
    weight_for_0 = total_samples / (2 * class_counts[0])
    weight_for_1 = total_samples / (2 * class_counts[1])
    class_weights = {0: weight_for_0, 1: weight_for_1}

    # Train the model
    densenet201_model, history = train_densenet201_model(build_densenet201_model, x_train_fold, y_train_fold, x_val_fold, y_val_fold, 12, model_save_dir, class_weights)

    # Evaluate the model on validation data
    val_loss, val_accuracy, val_sensitivity, val_specificity, val_auc = densenet201_model.evaluate(x_val_fold, y_val_fold)
    print(f'Validation Loss: {val_loss}')
    print(f'Validation Accuracy: {val_accuracy}')
    print(f'Validation Sensitivity: {val_sensitivity}')
    print(f'Validation Specificity: {val_specificity}')
    print(f'Validation AUC: {val_auc}')

    # Store validation scores
    validation_scores.append((val_loss, val_accuracy, val_sensitivity, val_specificity, val_auc))

# Calculate average validation scores across folds
average_validation_scores = np.mean(validation_scores, axis=0)
print("Average Validation Scores:")
print(f"Loss: {average_validation_scores[0]}")
print(f"Accuracy: {average_validation_scores[1]}")
print(f"Sensitivity: {average_validation_scores[2]}")
print(f"Specificity: {average_validation_scores[3]}")
print(f"AUC: {average_validation_scores[4]}")


# Save model and training history
model_path = os.path.join(model_save_dir, "densenet201_model.h5")  # Ensure the correct model name here
densenet201_model.save(model_path)  # Make sure to use the correct variable (densenet121_model)

history_path = os.path.join(model_save_dir, "densenet201_model_history.pkl")
with open(history_path, 'wb') as f:
    pickle.dump(history.history, f)  # Ensure using the correct history variable

print(f"DenseNet201 Model and training history saved to {model_save_dir}")

#Plotting the model results

#Getting the accuracy
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

#Getting the losses
loss = history.history['loss']
val_loss = history.history['val_loss']

#No of epochs it trained
epochs_range = history.epoch

#Plotting Training and Validation accuracy
plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('DenseNet201: Training and Validation Accuracy')

#Plotting Training and Validation Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('DenseNet201: Training and Validation Loss')
plt.show()

import os
import pickle

# Create a new folder for cross-validation results
model_save_dir = "/content/drive/MyDrive/KaggleData/XVal"
os.makedirs(model_save_dir, exist_ok=True)

# Save model and training history
model_path = os.path.join(model_save_dir, "densenet201.h5")  # Ensure the correct model name here
densenet201_model.save(model_path)  # Make sure to use the correct variable (densenet121_model)

history_path = os.path.join(model_save_dir, "densenet201_history.pkl")
with open(history_path, 'wb') as f:
    pickle.dump(history.history, f)  # Ensure using the correct history variable

print(f"densenet201 Model and training history saved to {model_save_dir}")

import matplotlib.pyplot as plt

# Assume 'history' is the object returned by the fit method of a model, you should use history.history to access the metrics
history_dict = history.history

# Check if the history_dict contains necessary data
if history_dict and 'accuracy' in history_dict and 'val_accuracy' in history_dict:
    # Extract the number of epochs from the history_dict
    epochs_range = range(len(history_dict['accuracy']))

    # Plotting Training and Validation accuracy
    plt.figure(figsize=(14, 5))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, history_dict['accuracy'], label='Training Accuracy')
    plt.plot(epochs_range, history_dict['val_accuracy'], label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('DenseNet201: Training and Validation Accuracy')

    # Plotting Training and Validation loss
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, history_dict['loss'], label='Training Loss')
    plt.plot(epochs_range, history_dict['val_loss'], label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('DenseNet201: Training and Validation Loss')

    plt.show()
else:
    print("The necessary training metrics are not available in the loaded history_dict.")

from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
import numpy as np

def calculate_f1_scores(probabilities, true_labels):
    """Calculates F1 scores for a range of thresholds."""
    thresholds = np.arange(0, 1, 0.01)
    f1_scores = [f1_score(true_labels, probabilities > threshold) for threshold in thresholds]
    return thresholds, f1_scores

def find_optimal_threshold(probabilities, true_labels):
    """Finds the threshold that yields the highest F1 score."""
    thresholds, f1_scores = calculate_f1_scores(probabilities, true_labels)
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    optimal_f1_score = f1_scores[optimal_idx]
    return optimal_threshold, optimal_f1_score

# # Extract images and labels
# images = np.array([i[0] for i in full_data])
# labels = np.array([i[1] for i in full_data])

# # Split the dataset into training and testing sets
# X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# # Assuming X_train and Y_train are already defined and preprocessed appropriately
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

predicted_probs = []
actual_labels = []

# Iterate over each fold
for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predicted probabilities from the model for the validation set
    # Reduce the batch size in your predict call
    probs = densenet201_model.predict(x_val_fold, batch_size=16).flatten()  # Adjust the batch size based on your available GPU memory

    predicted_probs.extend(probs)
    actual_labels.extend(y_val_fold)

# Convert lists to numpy arrays for analysis
predicted_probs = np.array(predicted_probs)
actual_labels = np.array(actual_labels)

# Find the optimal threshold and F1 score

optimal_threshold, optimal_f1_score = find_optimal_threshold(predicted_probs, actual_labels)
print(f"Optimal threshold: {optimal_threshold}")
print(f"Optimal F1 Score: {optimal_f1_score}")

# Plot F1 scores across thresholds
thresholds, f1_scores = calculate_f1_scores(predicted_probs, actual_labels)
plt.plot(thresholds, f1_scores)
plt.xlabel('Threshold')
plt.ylabel('F1 Score')
plt.title('DenseNet201: Threshold vs F1 Score')
plt.axvline(x=optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold = {optimal_threshold:.2f}')
plt.legend()
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf
from sklearn.model_selection import StratifiedKFold

# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# To store metrics for each fold
accuracies = []
precisions = []
recalls = []
f1_scores = []
roc_aucs = []
specificities = []

for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predictions
    y_pred_prob = densenet201_model.predict(x_val_fold)
    y_pred = np.round(y_pred_prob)

    # Calculate metrics
    accuracies.append(accuracy_score(y_val_fold, y_pred))
    precisions.append(precision_score(y_val_fold, y_pred))
    recalls.append(recall_score(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    roc_aucs.append(roc_auc_score(y_val_fold, y_pred_prob))

    # Specificity calculation
    tn, fp, fn, tp = confusion_matrix(y_val_fold, y_pred).ravel()
    specificities.append(tn / (tn + fp))


# Plot ROC curve and Precision-Recall curve for the last fold as an example
fpr, tpr, _ = roc_curve(y_val_fold, y_pred_prob)
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_aucs[-1]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('DenseNet201: ROC Curve')
plt.legend(loc='lower right')
plt.show()

precision, recall, _ = precision_recall_curve(y_val_fold, y_pred_prob)
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('DenseNet201: Precision-Recall Curve')
plt.show()

# Display Confusion Matrix for the last fold as an example
cm = confusion_matrix(y_val_fold, y_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('DenseNet201: Confusion Matrix')
plt.show()

# Display averaged metrics
print(f'Average Accuracy: {np.mean(accuracies)}')
print(f'Average Precision: {np.mean(precisions)}')
print(f'Average Recall: {np.mean(recalls)}')
print(f'Average Specificity: {np.mean(specificities)}')
print(f'Average F1-score: {np.mean(f1_scores)}')
print(f'Average ROC-AUC: {np.mean(roc_aucs)}')

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf
from sklearn.model_selection import StratifiedKFold

# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# To store metrics and additional data for each fold
accuracies = []
precisions = []
recalls = []
f1_scores = []
roc_aucs = []
specificities = []
all_fpr = []
all_tpr = []
all_precision = []
all_recall = []
conf_matrices = []

for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predictions
    y_pred_prob = densenet201_model.predict(x_val_fold)
    y_pred = np.round(y_pred_prob)

    # Calculate metrics
    accuracies.append(accuracy_score(y_val_fold, y_pred))
    precisions.append(precision_score(y_val_fold, y_pred))
    recalls.append(recall_score(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    roc_aucs.append(roc_auc_score(y_val_fold, y_pred_prob))

    # Specificity calculation
    tn, fp, fn, tp = confusion_matrix(y_val_fold, y_pred).ravel()
    specificities.append(tn / (tn + fp))

    # Store ROC curve data
    fpr, tpr, _ = roc_curve(y_val_fold, y_pred_prob)
    all_fpr.append(fpr)
    all_tpr.append(tpr)

    # Store Precision-Recall curve data
    precision, recall, _ = precision_recall_curve(y_val_fold, y_pred_prob)
    all_precision.append(precision)
    all_recall.append(recall)

    # Store confusion matrix
    conf_matrices.append(confusion_matrix(y_val_fold, y_pred))

# Plot ROC curves for all folds
plt.figure(figsize=(10, 8))
for i in range(n_splits):
    plt.plot(all_fpr[i], all_tpr[i], label=f'Fold {i+1} (AUC = {roc_aucs[i]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('DenseNet201: ROC Curves Across Folds')
plt.legend(loc='lower right')
plt.show()

# Plot Precision-Recall curves for all folds
plt.figure(figsize=(10, 8))
for i in range(n_splits):
    plt.plot(all_recall[i], all_precision[i], marker='.', label=f'Fold {i+1}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('DenseNet201: Precision-Recall Curves Across Folds')
plt.legend()
plt.show()

# Plot confusion matrices for each fold
fig, axes = plt.subplots(1, n_splits, figsize=(20, 4))
for i in range(n_splits):
    sns.heatmap(conf_matrices[i], annot=True, fmt='d', ax=axes[i])
    axes[i].set_xlabel('Predicted')
    axes[i].set_ylabel('Actual')
    axes[i].set_title(f'Fold {i+1} Confusion Matrix')
plt.show()

# Display averaged metrics
print(f'Average Accuracy: {np.mean(accuracies)}')
print(f'Average Precision: {np.mean(precisions)}')
print(f'Average Recall: {np.mean(recalls)}')
print(f'Average Specificity: {np.mean(specificities)}')
print(f'Average F1-score: {np.mean(f1_scores)}')
print(f'Average ROC-AUC: {np.mean(roc_aucs)}')

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import brier_score_loss
from sklearn.metrics import log_loss

# # Initialize StratifiedKFold
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store per-fold metrics
all_ece = []
all_mce = []
all_brier_scores = []
all_log_losses = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Generate predicted probabilities from the model for the validation set
    y_pred_probs = densenet201_model.predict(x_val_fold).flatten()

    # Calculate Brier score and log loss
    brier_score = brier_score_loss(y_val_fold, y_pred_probs)
    log_loss_score = log_loss(y_val_fold, y_pred_probs)
    all_brier_scores.append(brier_score)
    all_log_losses.append(log_loss_score)

    # Calibration curve and ECE/MCE calculations
    n_bins = 10
    bin_edges = np.linspace(0, 1, n_bins + 1)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

    bin_counts = np.histogram(y_pred_probs, bins=bin_edges)[0]
    bin_correct = np.histogram(y_pred_probs[y_val_fold == 1], bins=bin_edges)[0]
    bin_scores = np.histogram(y_pred_probs, bins=bin_edges, weights=y_pred_probs)[0]

    bin_accuracies = np.where(bin_counts > 0, bin_correct / bin_counts, 0)
    bin_confidences = np.where(bin_counts > 0, bin_scores / bin_counts, 0)

    ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_counts) / y_val_fold.size
    mce = np.max(np.abs(bin_accuracies - bin_confidences))

    all_ece.append(ece)
    all_mce.append(mce)

# Average the metrics across all folds
avg_brier_score = np.mean(all_brier_scores)
avg_log_loss = np.mean(all_log_losses)
avg_ece = np.mean(all_ece)
avg_mce = np.mean(all_mce)

print(f'Average Brier Score: {avg_brier_score:.4f}')
print(f'Average Log Loss: {avg_log_loss:.4f}')
print(f'Average ECE: {avg_ece:.4f}')
print(f'Average MCE: {avg_mce:.4f}')

# You may plot the average or last fold's calibration as an example
plt.figure(figsize=(10, 8))
plt.plot(bin_centers, bin_accuracies, 's-', label=f'Model Calibration (ECE={avg_ece:.4f}, MCE={avg_mce:.4f})')
plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')
plt.title('DenseNet201: Average Calibration Curve Across Folds')
plt.xlabel('Average Predicted Probability in Each Bin')
plt.ylabel('Fraction of Positives')
plt.legend()
plt.show()

from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import calibration_curve, IsotonicRegression
from sklearn.metrics import brier_score_loss, log_loss
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

# # Initialize StratifiedKFold
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store per-fold metrics
all_metrics = {
    'original': [],
    'platt': [],
    'isotonic': []
}

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]

    # Predict probabilities on the validation fold
    val_probs = densenet201_model.predict(x_val_fold).ravel()

    # Prepare binary arrays for the validation fold
    y_val_binary = np.argmax(y_val_fold, axis=1) if y_val_fold.ndim > 1 else y_val_fold.flatten()

    # Fit Platt Scaling on validation probabilities
    platt_scaler = LogisticRegression(solver='liblinear')
    platt_scaler.fit(val_probs.reshape(-1, 1), y_val_binary)
    calibrated_probs_platt = platt_scaler.predict_proba(val_probs.reshape(-1, 1))[:, 1]

    # Fit and apply Isotonic Regression for calibration on validation probabilities
    isotonic_scaler = IsotonicRegression(out_of_bounds='clip')
    isotonic_scaler.fit(val_probs, y_val_binary)
    calibrated_probs_isotonic = isotonic_scaler.transform(val_probs)

    # Store metrics and probabilities for each fold
    for label, probs in [('original', val_probs), ('platt', calibrated_probs_platt), ('isotonic', calibrated_probs_isotonic)]:
        brier = brier_score_loss(y_val_binary, probs)
        logloss = log_loss(y_val_binary, probs)
        all_metrics[label].append({
            'brier_score': brier,
            'log_loss': logloss,
            'probs': probs
        })

# Average metrics across all folds
for method in all_metrics.keys():
    avg_brier = np.mean([m['brier_score'] for m in all_metrics[method]])
    avg_log_loss = np.mean([m['log_loss'] for m in all_metrics[method]])
    print(f"{method} - Average Brier Score: {avg_brier:.4f}, Average Log Loss: {avg_log_loss:.4f}")

# Plot calibration curves for the test set from the last fold as an example
plt.figure(figsize=(8, 6))
for method in all_metrics.keys():
    prob_true, prob_pred = calibration_curve(y_val_binary, all_metrics[method][-1]['probs'], n_bins=10)
    plt.plot(prob_pred, prob_true, marker='o', linestyle='-', label=method)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')
plt.xlabel('Mean predicted probability')
plt.ylabel('Fraction of positives')
plt.title('DenseNet201: Calibration Curves (Validation Set)')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import StratifiedKFold

# # Initialize StratifiedKFold
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store per-fold results
all_cm_original = []
all_cm_platt = []
all_cm_isotonic = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]


    # Generate predicted probabilities and apply calibrations
    val_probs = densenet201_model.predict(x_val_fold).ravel()
    calibrated_probs_platt = platt_scaler.predict_proba(val_probs.reshape(-1, 1))[:, 1]  # Assuming platt_scaler is already fitted in the cross-val setup
    calibrated_probs_isotonic = isotonic_scaler.transform(val_probs)  # Assuming isotonic_scaler is fitted

    # Define a threshold for binary classification
    threshold = 0.41

    # Convert probabilities to binary classifications
    predicted_classes_original = (val_probs >= threshold).astype(int)
    predicted_classes_platt = (calibrated_probs_platt >= threshold).astype(int)
    predicted_classes_isotonic = (calibrated_probs_isotonic >= threshold).astype(int)

    # Compute confusion matrices
    cm_original = confusion_matrix(y_val_fold, predicted_classes_original)
    cm_platt = confusion_matrix(y_val_fold, predicted_classes_platt)
    cm_isotonic = confusion_matrix(y_val_fold, predicted_classes_isotonic)

    # Store confusion matrices
    all_cm_original.append(cm_original)
    all_cm_platt.append(cm_platt)
    all_cm_isotonic.append(cm_isotonic)

    # Optionally print classification reports here or after the loop

# Function to plot confusion matrices
def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(title)
    plt.show()

# Example plotting of the first fold results (or you could average the CM values)
plot_confusion_matrix(all_cm_original[0], 'DenseNet201: Confusion Matrix (Original)')
plot_confusion_matrix(all_cm_platt[0], 'DenseNet201: Confusion Matrix (Platt Scaling)')
plot_confusion_matrix(all_cm_isotonic[0], 'DenseNet201: Confusion Matrix (Isotonic Regression)')

# Example classification report (for the first fold)
print('DenseNet201: Classification Report - Original:')
print(classification_report(y_val_fold, predicted_classes_original))
print('DenseNet201: Classification Report - Platt Scaling:')
print(classification_report(y_val_fold, predicted_classes_platt))
print('DenseNet201: Classification Report - Isotonic Regression:')
print(classification_report(y_val_fold, predicted_classes_isotonic))

def plot_predictions_cross_val(model, dataset, class_names, num_images=28):
    plt.figure(figsize=(14, 10))
    plt.suptitle('DenseNet201: Model Predictions (Cross-Validation)', fontsize=16)

    # Shuffle and take a batch from the dataset
    for images, labels in dataset.shuffle(buffer_size=1000).take(1):  # Shuffle here for randomness
        predictions = model.predict(images)
        predictions = predictions.ravel()  # Flatten the predictions

        # Limit the number of images to plot (in case the last batch has fewer than num_images)
        num_images = min(num_images, images.shape[0])

        # Plot each image in the batch
        for i in range(num_images):
            ax = plt.subplot(4, 7, i + 1)  # Adjust the subplot grid if you change num_images
            plt.imshow(images[i].numpy().astype("uint8"))
            plt.axis('off')

            # Predicted label
            predicted_index = int(predictions[i] >= 0.41)  # Thresholding for binary classification
            true_index = labels[i].numpy()

            # Title with predicted and true labels
            title = f"Pred: {class_names[predicted_index]}\nTrue: {class_names[true_index]}"
            plt.title(title, color='green' if predicted_index == true_index else 'red', fontsize=9)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to make room for the suptitle
    plt.show()


# # Define the number of splits for cross-validation
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize list to store validation datasets for each fold
val_datasets = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = Y_train[train_index], Y_train[val_index]

    # Convert validation data to TensorFlow Dataset
    val_dataset_fold = tf.data.Dataset.from_tensor_slices((x_val_fold, y_val_fold))
    val_dataset_fold = val_dataset_fold.batch(32)  # Adjust batch size as needed

    # Store the validation dataset for the current fold
    val_datasets.append(val_dataset_fold)

    # Plot predictions for the current fold
    print(f"Fold {len(val_datasets)} Predictions:")
    class_names = ['Benign', 'Malignant']  # Modify as per your classes
    plot_predictions_cross_val(densenet201_model, val_dataset_fold, class_names)

import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
import numpy as np

def plot_predictions_cross_val(images, labels, predictions, class_names, num_images=28):
    plt.figure(figsize=(14, 10))
    plt.suptitle('DenseNet201: Model Predictions (Cross-Validation)', fontsize=16)

    # Ensure not to exceed the array length
    num_images = min(num_images, len(images))

    indices = np.random.choice(range(len(images)), num_images, replace=False)  # Randomly select images to display

    for i, idx in enumerate(indices):
        ax = plt.subplot(4, 7, i + 1)  # Adjust the subplot grid if you change num_images
        plt.imshow(images[idx].astype("uint8"))
        plt.axis('off')

        predicted_index = int(predictions[idx] >= 0.41)  # Threshold for binary classification
        true_index = labels[idx]

        # Title with predicted and true labels
        title = f"Pred: {class_names[predicted_index]}\nTrue: {class_names[true_index]}"
        plt.title(title, color='green' if predicted_index == true_index else 'red', fontsize=9)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to make room for the suptitle
    plt.show()

# # Assuming you have a function 'densenet121_model' for predictions and 'X_train', 'Y_train' defined
# n_splits = 5
# kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

all_images = []
all_labels = []
all_predictions = []

# Loop over each fold for cross-validation
for train_index, val_index in kf.split(X_train, Y_train):
    x_val_fold = X_train[val_index]
    y_val_fold = Y_train[val_index]

    # Predictions for the fold
    predictions = densenet201_model.predict(x_val_fold).ravel()  # Ensure model and method are correctly referenced

    # Store results
    all_images.extend(x_val_fold)
    all_labels.extend(y_val_fold)
    all_predictions.extend(predictions)

# Now plot predictions for all collected data from folds
class_names = ['Benign', 'Malignant']  # Modify as per your classes
plot_predictions_cross_val(np.array(all_images), np.array(all_labels), np.array(all_predictions), class_names)



"""# **VGG16**"""
